{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5007e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "# For limiting GPU VRAM used by process\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7500)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58285d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.getcwd() + os.sep + 'te' + os.sep + 'lexicons'\n",
    "train_path = dataset_path + os.sep + 'te.translit.sampled.train.tsv'\n",
    "valid_path = dataset_path + os.sep + 'te.translit.sampled.dev.tsv'\n",
    "test_path = dataset_path + os.sep + 'te.translit.sampled.test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = []\n",
    "train_outputs = []\n",
    "valid_inputs = []\n",
    "valid_outputs = []\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "input_chars = set()\n",
    "output_chars = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad91d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: (len(lines) - 1 )]:\n",
    "    out,inp,a = line.split('\\t')\n",
    "#     if not include_all and a!=1:\n",
    "#         continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     print(out,out[-1],inp,inp[-1])\n",
    "    out = \"\\t\" + out + \"\\n\"\n",
    "    train_inputs.append(inp)\n",
    "    train_outputs.append(out)\n",
    "    for char in inp:\n",
    "        if char not in input_chars:\n",
    "            input_chars.add(char)\n",
    "    for char in out:\n",
    "        if char not in output_chars:\n",
    "            output_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77599cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: (len(lines) - 1 )]:\n",
    "    out,inp,a = line.split('\\t')\n",
    "    if not include_all and a!=1:\n",
    "        continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     print(out,out[-1],inp,inp[-1])\n",
    "    out = \"\\t\" + out + \"\\n\"\n",
    "    valid_inputs.append(inp)\n",
    "    valid_outputs.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56668b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: (len(lines) - 1 )]:\n",
    "    out,inp,a = line.split('\\t')\n",
    "    if not include_all and a!=1:\n",
    "        continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     print(out,out[-1],inp,inp[-1])\n",
    "    out = \"\\t\" + out + \"\\n\"\n",
    "    test_inputs.append(inp)\n",
    "    test_outputs.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e91aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chars = sorted(list(input_chars))\n",
    "print(input_chars)\n",
    "num_input_chars = len(input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_chars = sorted(list(output_chars))\n",
    "print(output_chars)\n",
    "num_output_chars = len(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_inputs)\n",
    "max_input_size = max([len(txt) for txt in train_inputs])\n",
    "max_valid_input_size = max([len(txt) for txt in valid_inputs])\n",
    "max_test_input_size = max([len(txt) for txt in test_inputs])\n",
    "print(max_input_size,max_valid_input_size,max_test_input_size)\n",
    "max_output_size = max([len(txt) for txt in  train_outputs])\n",
    "print(max_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index = dict([(char, i+1) for i, char in enumerate(input_chars)])\n",
    "output_index = dict([(char, i+1) for i, char in enumerate(output_chars)])\n",
    "print(input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enoding in indexes of characters in the set\n",
    "def encode_index(inputs,index):\n",
    "    data = []\n",
    "    for i in range(len(inputs)):\n",
    "        a = np.zeros(len(inputs[i]))\n",
    "        j = 0\n",
    "        for char in inputs[i]:\n",
    "            a[j] = index[char]\n",
    "            j += 1\n",
    "        data.append(a)\n",
    "    data = np.asarray(data).astype(np.ndarray)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = encode_index(train_inputs,input_index)\n",
    "input_tensor = tf.ragged.constant(input_data).to_tensor()\n",
    "\n",
    "val_input_data = encode_index(valid_inputs,input_index)\n",
    "val_input_tensor = tf.ragged.constant(val_input_data).to_tensor()\n",
    "\n",
    "test_input_data = encode_index(test_inputs,input_index)\n",
    "test_input_tensor = tf.ragged.constant(test_input_data).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953483ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350640cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val__input_size = max([len(txt) for txt in valid_inputs])\n",
    "max_val_output_size = max([len(txt) for txt in  valid_outputs])\n",
    "max_test_input_size = max([len(txt) for txt in test_inputs])\n",
    "max_test_output_size = max([len(txt) for txt in  test_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = np.zeros(\n",
    "    (len(train_inputs), max_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(train_inputs), max_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "for i,target_text in enumerate(train_outputs):\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, output_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_output_data[i, t - 1, output_index[char]] = 1.0\n",
    "#     decoder_input_data[i, t + 1 :, output_index[\" \"]] = 1.0\n",
    "#     decoder_output_data[i, t:, output_index[\" \"]] = 1.0\n",
    "# print(decoder_input_data[0])\n",
    "decoder_input_data = np.argmax(decoder_input_data,axis=2).astype(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32549f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_val_input_data = np.zeros(\n",
    "    (len(valid_inputs), max_val_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "decoder_val_output_data = np.zeros(\n",
    "    (len(valid_inputs), max_val_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "for i,target_text in enumerate(valid_outputs):\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_val_input_data[i, t, output_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_val_output_data[i, t - 1, output_index[char]] = 1.0\n",
    "#     decoder_input_data[i, t + 1 :, output_index[\" \"]] = 1.0\n",
    "#     decoder_output_data[i, t:, output_index[\" \"]] = 1.0\n",
    "decoder_val_input_data = np.argmax(decoder_val_input_data,axis=2).astype(dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f20581",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_test_input_data = np.zeros(\n",
    "    (len(test_inputs), max_test_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "decoder_test_output_data = np.zeros(\n",
    "    (len(test_inputs), max_test_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "for i,target_text in enumerate(test_outputs):\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_test_input_data[i, t, output_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_test_output_data[i, t - 1, output_index[char]] = 1.0\n",
    "#     decoder_input_data[i, t + 1 :, output_index[\" \"]] = 1.0\n",
    "#     decoder_output_data[i, t:, output_index[\" \"]] = 1.0\n",
    "decoder_test_input_data = np.argmax(decoder_test_input_data,axis=2).astype(dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751cf97",
   "metadata": {},
   "source": [
    "# Adding Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2157e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb688c59",
   "metadata": {},
   "source": [
    "# Sample Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sample_model(input_embed_size , hidden_size):\n",
    "#     charinput = tf.keras.Input(shape=(None,),dtype='float32',name=\"input\")\n",
    "#     embedding = tf.keras.layers.Embedding(num_input_chars+1,input_embed_size, name=\"embedding\")(charinput)\n",
    "    \n",
    "#     encoder = tf.keras.layers.LSTM(hidden_size, return_state=True,return_sequences=True )\n",
    "#     encoder_outputs, state_h, state_c = encoder(embedding)\n",
    "#     encoder_states = [state_h, state_c]\n",
    "    \n",
    "#     decoder_inputs = tf.keras.Input(shape=(None,),name=\"decoder_input\")\n",
    "#     decoder_embedding = tf.keras.layers.Embedding(num_output_chars + 1,input_embed_size, name=\"decoder_embedding\",mask_zero=True)(decoder_inputs)\n",
    "    \n",
    "#     decoder_lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "#     decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "#     print(encoder_outputs.shape)\n",
    "#     # Attention layer\n",
    "#     attn_layer = AttentionLayer(name='attention_layer')\n",
    "#     attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "#     # Concat attention input and decoder GRU output\n",
    "#     decoder_concat_input = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attn_out])\n",
    "\n",
    "#     # Dense layer\n",
    "#     decoder_dense = tf.keras.layers.Dense(num_output_chars + 1, activation=\"softmax\")\n",
    "#     dense_time = tf.keras.layers.TimeDistributed(decoder_dense)\n",
    "# #     decoder_pred = dense_time(decoder_concat_input)\n",
    "    \n",
    "#     decoder_outputs = dense_time(decoder_concat_input)\n",
    "    \n",
    "#     model = tf.keras.Model([charinput,decoder_inputs],decoder_outputs)\n",
    "    \n",
    "#     encoder_states_attn = [encoder_outputs,state_h,state_c]\n",
    "#     encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "#     # define inference decoder\n",
    "# #     decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "# #     attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "# #     decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "# #     decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "# #     decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "# #                           outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "    \n",
    "#     decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "#     decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "#     decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "#     encoder_inf_states = tf.keras.Input(shape=(None,hidden_size))\n",
    "#     decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "#     attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "#     decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attn_inf_out])\n",
    "#     decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "#     decoder_states = [state_h, state_c]\n",
    "    \n",
    "#     decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h,decoder_state_input_c]\n",
    "#                                    , [decoder_outputs] + decoder_states + [attn_inf_states])\n",
    "#     return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model, enc_model, dec_model = get_sample_model(16,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model.compile(\n",
    "#     optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "# sample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc12abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_input_char_index = dict((i, char) for char, i in input_index.items())\n",
    "# reverse_target_char_index = dict((i, char) for char, i in output_index.items())\n",
    "# reverse_target_char_index[0] = ' '\n",
    "\n",
    "# def decode_sequence(input_seq):\n",
    "#     sz  = input_seq.shape[0]\n",
    "#     states_value = enc_model.predict(input_seq)\n",
    "#     target_seq = np.zeros((sz,1,num_output_chars+1))\n",
    "#     for i in range(sz):\n",
    "#         target_seq[i, 0, output_index[\"\\t\"]] = 1.0\n",
    "#     target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "#     decoded_seqs = [\"\" for i in range(sz)]\n",
    "#     j = 0\n",
    "#     while j < max_output_size:\n",
    "#         output_tokens, h, c, attn = dec_model.predict([target_seq] + states_value)\n",
    "\n",
    "# #         print(output_tokens)\n",
    "#         sampled_token_index = np.argmax(output_tokens[:, -1, :],axis=1)\n",
    "#         target_seq = np.zeros((sz, 1, num_output_chars+1))\n",
    "#         for i in range(sz):\n",
    "#             sampled_char = reverse_target_char_index[sampled_token_index[i]]\n",
    "#             decoded_seqs[i] += sampled_char\n",
    "#             target_seq[i, 0, sampled_token_index[i]] = 1.0\n",
    "#         target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "#         # Update states\n",
    "#         states_value[1] = h\n",
    "#         states_value[2] = c\n",
    "#         j+=1\n",
    "#     output = [ (\"\\t\"+st.split('\\n')[0]+\"\\n\") for st in decoded_seqs]\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = input_tensor.to_tensor()\n",
    "# dec_model.run_eagerly = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cea938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seqid in range(10):\n",
    "#     input_seq = input_tensor[seqid:seqid+1]\n",
    "# #     print(input_seq.shape,input_tensor.shape)\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "#     print(\"-\")\n",
    "#     print(\"Input sentence:\", train_inputs[seqid])\n",
    "#     print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f42ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model.fit(\n",
    "#     [input_tensor,decoder_input_data],\n",
    "#     decoder_output_data,\n",
    "#     batch_size=64,\n",
    "#     epochs=5,\n",
    "#     validation_data=([val_input_tensor,decoder_val_input_data],decoder_val_output_data),\n",
    "#     shuffle=True,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(data_tensor,data_output,k):\n",
    "#     crct = 0\n",
    "#     input_seq = data_tensor[:k]\n",
    "# #     print(input_seq.shape,input_tensor.shape)\n",
    "#     decoded_sentences = decode_sequence(input_seq)\n",
    "#     sts = data_output[:k]\n",
    "#     crct += np.sum(np.array(sts) == np.array(decoded_sentences))\n",
    "# #         print(crct/(seqid+1))\n",
    "# #         for st,d in zip(sts,decoded_sentences):\n",
    "# #             print(st+\"_o\")\n",
    "# #             print(d+\"_o\")\n",
    "#     return crct/k,zip(decoded_sentences,sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d57cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = evaluate(test_input_tensor,test_outputs,len(test_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a59bc",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in output_index.items())\n",
    "reverse_target_char_index[0] = ' '\n",
    "\n",
    "def beam_decode(input_seq, beam_size, enc_model, dec_model, cell_type):\n",
    "    sz  = input_seq.shape[0]\n",
    "    attention = []\n",
    "    states_value = enc_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((sz,1,num_output_chars+1))\n",
    "    \n",
    "    for i in range(sz):\n",
    "        target_seq[i, 0, output_index[\"\\t\"]] = 1.0\n",
    "    \n",
    "    target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "    decoded_seqs = [\"\" for i in range(sz)]\n",
    "    if cell_type == 'LSTM':\n",
    "        output_tokens, h, c, attn = dec_model.predict([target_seq] + states_value)\n",
    "        states = [states_value[0],h,c]\n",
    "        attention.append(attn)\n",
    "    if cell_type == 'GRU' or cell_type == 'RNN':\n",
    "        output_tokens, h, attn = dec_model.predict([target_seq] + states_value)\n",
    "        states = [states_value[0],h]\n",
    "        attention.append(attn)\n",
    "    \n",
    "    sequences = []\n",
    "    for i in range(sz):\n",
    "        sequences.append([])\n",
    "    sampled_token_beam = np.argpartition(output_tokens[:, -1, :], -beam_size ,axis=1)[:,-beam_size:]\n",
    "    sampled_token_indexes = np.argmax(output_tokens[:, -1, :],axis=1)\n",
    "    for i in range(sz):\n",
    "        allcandidates = list()\n",
    "        for j in range(beam_size):\n",
    "            allcandidates.append(\n",
    "                    [ [ sampled_token_beam[i][j] ],\n",
    "                        -np.log( \n",
    "                        output_tokens[i][-1][sampled_token_beam[i][j]]),\n",
    "                        states ,\n",
    "                        False])\n",
    "        ordered = sorted(allcandidates, key=lambda tup:tup[1])\n",
    "        sequences[i] = ordered[:beam_size]\n",
    "        \n",
    "    target_seq = np.zeros((sz, beam_size, num_output_chars+1))\n",
    "    for i in range(sz):\n",
    "        for j in range(beam_size): \n",
    "            target_seq[i, j, sequences[i][j][0][-1]] = 1.0\n",
    "    target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "    it = 1\n",
    "    while it < max_output_size:\n",
    "        allcandidates = [list() for i in range(sz)]\n",
    "        for k in range(len(sequences[i])):\n",
    "            if cell_type == 'LSTM':\n",
    "                output_tokens, h, c, attn = dec_model.predict(\n",
    "                [target_seq[:,k]] + \n",
    "                sequences[i][k][2])\n",
    "                states = [states_value[0],h,c]\n",
    "                attention.append(attn)\n",
    "            if cell_type == 'GRU' or cell_type == 'RNN':\n",
    "                output_tokens, h, attn = dec_model.predict(\n",
    "                [target_seq[:,k]] + \n",
    "                sequences[i][k][2])\n",
    "                states = [states_value[0],h]\n",
    "                attention.append(attn)\n",
    "            sampled_token_beam = np.argpartition(output_tokens[:, -1, :], -beam_size ,axis=1)[:,-beam_size:]\n",
    "            sampled_token_indexes = np.argmax(output_tokens[:, -1, :],axis=1)\n",
    "            \n",
    "            for i in range(sz):\n",
    "                    if sequences[i][k][3]:\n",
    "                        allcandidates[i].append(\n",
    "                                [ sequences[i][k][0]+[ sampled_token_beam[i][j] ],\n",
    "                                 sequences[i][k][1],\n",
    "                                           states, True ])\n",
    "                        continue\n",
    "                    for j in range(beam_size):\n",
    "                        if reverse_target_char_index[sampled_token_beam[i][j]]=='\\n':\n",
    "                            allcandidates[i].append(\n",
    "                                [ sequences[i][k][0]+[ sampled_token_beam[i][j] ],\n",
    "                                 sequences[i][k][1]-np.log( \n",
    "                                     output_tokens[i][-1][sampled_token_beam[i][j]]),\n",
    "                                           states, True ])\n",
    "                        else:\n",
    "                            allcandidates[i].append(\n",
    "                            [ sequences[i][k][0]+[ sampled_token_beam[i][j] ],\n",
    "                             sequences[i][k][1]-np.log( \n",
    "                                 output_tokens[i][-1][sampled_token_beam[i][j]]),\n",
    "                                       states, False ])\n",
    "        for i in range(sz):\n",
    "            ordered = sorted(allcandidates[i], key=lambda tup:tup[1])\n",
    "            sequences[i] = ordered[:beam_size]\n",
    "        target_seq = np.zeros((sz, beam_size, num_output_chars+1))\n",
    "        for i in range(sz):\n",
    "            for j in range(beam_size): \n",
    "                target_seq[i, j, sequences[i][j][0][-1]] = 1.0\n",
    "        target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "        it+=1\n",
    "    output = []\n",
    "    for i in range(sz):\n",
    "        st = \"\"\n",
    "        for ind in sequences[i][0][0]:\n",
    "            st += reverse_target_char_index[ind]\n",
    "        output.append(\"\\t\"+st.split('\\n')[0]+\"\\n\")\n",
    "    return output , attention\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0075b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate(data_tensor,data_output,k,beam_size,enc_model, dec_model, cell_type):\n",
    "    crct = 0\n",
    "    input_seq = data_tensor[:k]\n",
    "    decoded_sentences, attention = beam_decode(input_seq,beam_size,enc_model, dec_model, cell_type)\n",
    "    sts = data_output[:k]\n",
    "    crct += np.sum(np.array(sts) == np.array(decoded_sentences))\n",
    "    return crct/k,zip(decoded_sentences,sts),attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1, b1, attention = beam_evaluate(test_input_tensor,test_outputs,len(test_outputs),1,enc_model,dec_model,'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779eecba",
   "metadata": {},
   "source": [
    "# Attention color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate_single(data_tensor,data_output,index,beam_size,enc_model, dec_model, cell_type):\n",
    "    crct = 0\n",
    "    input_seq = data_tensor[index:index+1]\n",
    "    decoded_sentences, attention = beam_decode(input_seq,beam_size,enc_model, dec_model, cell_type)\n",
    "    sts = data_output[index:index+1]\n",
    "    crct += np.sum(np.array(sts) == np.array(decoded_sentences))\n",
    "    return crct,zip(decoded_sentences,sts),attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f1b6b",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89312f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textualheatmap import TextualHeatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def data_string1(attention,output,input):\n",
    "    ## attention shape len(output) x len(input)  \n",
    "    data = []\n",
    "    dic_list = []\n",
    "    for i in range(len(output)):\n",
    "        l = []\n",
    "        a = []\n",
    "        for j in range(len(input)):\n",
    "            a.append((-math.log(attention[i][j]),input[j]))\n",
    "        a.sort()\n",
    "        m = []\n",
    "        for j in range(4):\n",
    "            if a[j][0] <= 3:\n",
    "                m.append(a[j][1])\n",
    "        d = dict()\n",
    "        d[\"token\"] =  output[i]\n",
    "        d[\"meta\"] = m\n",
    "        d[\"heat\"] = [1.0]\n",
    "        d1 = dict()\n",
    "        d1[\"token\"] = ' '\n",
    "        d1[\"format\"] = False\n",
    "        dic_list.append(d)\n",
    "        dic_list.append(d1)\n",
    "    data.append(dic_list)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_string2(attention,output,input):\n",
    "    ## attention shape len(output) x len(input)  \n",
    "    data = []\n",
    "    dic_list = []\n",
    "    i = 0\n",
    "    while i < (len(output)):\n",
    "        token = output[i]\n",
    "        a = []\n",
    "        for j in range(len(input)):\n",
    "            a.append((-math.log(attention[i][j]),input[j]))\n",
    "        a.sort()\n",
    "        m = []\n",
    "        \n",
    "        mst = \"root: \"\n",
    "        for j in range(4):\n",
    "            if a[j][0] <= 3:\n",
    "                mst = mst + a[j][1] + \" \"\n",
    "        m.append(mst)\n",
    "        while(  ( (i+1)<len(output) ) and ( output_index[output[i+1]] < 5 or output_index[output[i+1]] > 51) ):\n",
    "            i+=1\n",
    "            token += output[i]\n",
    "            a = []\n",
    "            for j in range(len(input)):\n",
    "                a.append((-math.log(attention[i][j]),input[j]))\n",
    "            a.sort()\n",
    "            mst = output[i] + \": \"\n",
    "            for j in range(4):\n",
    "                if a[j][0] <= 3:\n",
    "                    mst = mst + a[j][1] + \" \"\n",
    "            m.append(mst)\n",
    "\n",
    "        d = dict()\n",
    "        d[\"token\"] =  token\n",
    "        d[\"meta\"] = m\n",
    "        d[\"heat\"] = [1.0]\n",
    "        d1 = dict()\n",
    "        d1[\"token\"] = ' '\n",
    "        d1[\"format\"] = True\n",
    "        dic_list.append(d)\n",
    "        i+=1\n",
    "        dic_list.append(d1)\n",
    "    data.append(dic_list)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e8b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d420bcfd",
   "metadata": {},
   "source": [
    "# Wandb Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10baf71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login(key=\"866040d7d81f67025d43e7d50ecd83d54b6cf977\", relogin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_word_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "         'beam_size' : {\n",
    "            'values' : [1]\n",
    "        },\n",
    "        'input_embed_size': {\n",
    "            'values' : [32, 64]\n",
    "        },\n",
    "        'hidden_size' : {\n",
    "            'values' : [128, 256, 512]\n",
    "        },\n",
    "        'cell_type' : {\n",
    "            'values' : ['GRU','LSTM','RNN']\n",
    "        },\n",
    "        'dropout' : {\n",
    "            'values' : [0,0.2]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab47e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, entity=\"mooizz\",project=\"Rec_dakhashina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Model(input_embed_size , hidden_size, cell_type, dropout):\n",
    "    charinput = tf.keras.Input(shape=(None,),dtype='float32',name=\"input\")\n",
    "    embedding = tf.keras.layers.Embedding(num_input_chars+1,input_embed_size, name=\"embedding\")(charinput)\n",
    "    \n",
    "    if cell_type == 'LSTM':\n",
    "        encoder = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True )\n",
    "        encoder_outputs, state_h, state_c = encoder(embedding)\n",
    "        encoder_states = [state_h, state_c]\n",
    "    if cell_type == 'RNN':\n",
    "        encoder = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, return_state=True )\n",
    "        encoder_outputs, rnn_state = encoder(embedding)\n",
    "    if cell_type == 'GRU':\n",
    "        encoder = tf.keras.layers.GRU(hidden_size, return_sequences=True, return_state=True )\n",
    "        encoder_outputs, gru_state = encoder(embedding)\n",
    "        \n",
    "    \n",
    "    decoder_inputs = tf.keras.Input(shape=(None,),name=\"decoder_input\")\n",
    "    decoder_embedding = tf.keras.layers.Embedding(num_output_chars + 1,input_embed_size, name=\"decoder_embedding\",mask_zero=True)(decoder_inputs)\n",
    "    \n",
    "    if cell_type == 'LSTM':\n",
    "        decoder_lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    if cell_type == 'RNN':\n",
    "        decoder_rnn = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=rnn_state)\n",
    "    if cell_type == 'GRU':\n",
    "        decoder_gru = tf.keras.layers.GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _ = decoder_gru(decoder_embedding, initial_state=gru_state)\n",
    "    \n",
    "    # print(encoder_outputs.shape)\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    decoder_dense = tf.keras.layers.Dense(num_output_chars + 1, activation=\"softmax\")\n",
    "    dense_time = tf.keras.layers.TimeDistributed(decoder_dense)\n",
    "#     decoder_pred = dense_time(decoder_concat_input)\n",
    "    \n",
    "    decoder_outputs = dense_time(decoder_concat_input)\n",
    "    \n",
    "    model = tf.keras.Model([charinput,decoder_inputs],decoder_outputs)\n",
    "    \n",
    "    \n",
    "    if cell_type == 'LSTM':\n",
    "        encoder_states_attn = [encoder_outputs,state_h,state_c]\n",
    "        encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "        # define inference decoder\n",
    "    #     decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "    #     attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "    #     decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "    #     decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "    #     decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "    #                           outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "\n",
    "        decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "        decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "        encoder_inf_states = tf.keras.Input(shape=(None,hidden_size))\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "        attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "        decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attn_inf_out])\n",
    "        decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "        decoder_states = [state_h, state_c]\n",
    "\n",
    "        decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h,decoder_state_input_c]\n",
    "                                       , [decoder_outputs] + decoder_states + [attn_inf_states])\n",
    "    if cell_type == 'GRU':\n",
    "        encoder_states_attn = [encoder_outputs,gru_state]\n",
    "        \n",
    "        encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "        decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "        encoder_inf_states = tf.keras.Input(shape=(None,hidden_size))\n",
    "        \n",
    "        decoder_outputs, state_h = decoder_gru(decoder_embedding, initial_state=decoder_state_input_h)\n",
    "        attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "        decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attn_inf_out])\n",
    "        decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "#         decoder_states = [state_h, state_c]\n",
    "\n",
    "        decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h]\n",
    "                                       , [decoder_outputs] + [state_h] + [attn_inf_states])\n",
    "    if cell_type == 'RNN':\n",
    "        encoder_states_attn = [encoder_outputs,rnn_state]\n",
    "        \n",
    "        encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "        decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "        encoder_inf_states = tf.keras.Input(shape=(None,hidden_size))\n",
    "        \n",
    "        decoder_outputs, state_h = decoder_rnn(decoder_embedding, initial_state=decoder_state_input_h)\n",
    "        attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "        decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attn_inf_out])\n",
    "        decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "#         decoder_states = [state_h, state_c]\n",
    "\n",
    "        decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h]\n",
    "                                       , [decoder_outputs] + [state_h] + [attn_inf_states])\n",
    "        \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_defaults = {\n",
    "        'epochs' : 10,\n",
    "        'batch_size' : 64,\n",
    "        'optimizer' : 'adam',\n",
    "        'beam_size' : 1,\n",
    "        'input_embed_size': 32,\n",
    "        'hidden_size' : 256,\n",
    "        'cell_type' : 'LSTM',\n",
    "        'dropout' : 0.2,\n",
    "    }\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "    model, enc_model, dec_model = get_Model(config.input_embed_size,config.hidden_size,\n",
    "                     config.cell_type,\n",
    "                     config.dropout)\n",
    "    model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "#     sample_model.summary()\n",
    "    EarlyStopCB = tf.keras.callbacks.EarlyStopping(patience=30, monitor='val_accuracy',\n",
    "                                                  restore_best_weights=True)\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    model.fit(\n",
    "        [input_tensor,decoder_input_data],\n",
    "        decoder_output_data,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epochs,\n",
    "        validation_data=(\n",
    "            [val_input_tensor,decoder_val_input_data],\n",
    "            decoder_val_output_data\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        callbacks=[WandbCallback(), EarlyStopCB])\n",
    "    beam_acc , _ , attention = beam_evaluate(val_input_tensor,valid_outputs,len(valid_outputs),config.beam_size,\n",
    "                                enc_model,\n",
    "                                dec_model,\n",
    "                                config.cell_type)\n",
    "    wandb.log({'val_word_accuracy' : beam_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e625837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffe5d0",
   "metadata": {},
   "source": [
    "# Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init( entity=\"mooizz\",project=\"Rec_dakhashina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, enc_model, dec_model = get_Model(64,512,\n",
    "                     'LSTM',\n",
    "                     0.2)\n",
    "best_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopCB = tf.keras.callbacks.EarlyStopping(patience=30, monitor='val_accuracy',\n",
    "                                                  restore_best_weights=True)\n",
    "tf.config.run_functions_eagerly(True)\n",
    "best_model.fit(\n",
    "        [input_tensor,decoder_input_data],\n",
    "        decoder_output_data,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        validation_data=(\n",
    "            [val_input_tensor,decoder_val_input_data],\n",
    "            decoder_val_output_data\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        callbacks=[WandbCallback(), EarlyStopCB]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_acc , _ , attention = beam_evaluate(val_input_tensor,valid_outputs,len(valid_outputs),1,\n",
    "                                enc_model,\n",
    "                                dec_model,\n",
    "                                'LSTM')\n",
    "wandb.log({'val_word_accuracy' : beam_acc})"
   ]
  },
  {
   "source": [
    "Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_acc , pred , attention = beam_evaluate(test_input_tensor,test_outputs,len(test_outputs),1,\n",
    "                                enc_model,\n",
    "                                dec_model,\n",
    "                                'LSTM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beam_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular format\n",
    "from tabulate import tabulate\n",
    "i = 0\n",
    "data = []\n",
    "for a,b in pred:\n",
    "    l = []\n",
    "    l.append(test_inputs[i])\n",
    "    l.append(a[1:-1])\n",
    "    l.append(b[1:-1])\n",
    "    \n",
    "    i+=1\n",
    "    data.append(l)\n",
    "import csv\n",
    "with open('predictions_attention.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Input Word\", \"Predicted Word\", \"True Word\"])\n",
    "    for i in range(len(data)):\n",
    "      writer.writerow(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('predictions_attention.csv')\n",
    "frame = df.sample(frac=1).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'predictions': wandb.Table(dataframe=frame)})"
   ]
  },
  {
   "source": [
    "Visualizing HeatMaps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "random_indexes = []\n",
    "# outputs = []\n",
    "# inputs = []\n",
    "for i in range(9):\n",
    "    a = np.random.randint(0,len(test_outputs))\n",
    "    random_indexes.append(a)\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "x = 1\n",
    "for i in range(9):\n",
    "    accruracy , z, attention = beam_evaluate_single(test_input_tensor,test_outputs,random_indexes[i],1,enc_model,dec_model,'LSTM')\n",
    "    # print(attention)\n",
    "    attention = np.asarray(np.asarray(attention)[:,0,:])\n",
    "    s = attention.shape\n",
    "    attention = attention.reshape(s[0],s[2])[:,:s[0]]\n",
    "    plt.subplot(3,3,x)\n",
    "    # print(attention.shape,attention)\n",
    "    for a, b in z:\n",
    "        decoded_word = a[1:-1]\n",
    "        expected_word = b \n",
    "    input_word = test_inputs[random_indexes[i]]\n",
    "    print(len(input_word),input_word)\n",
    "    print(len(decoded_word),decoded_word)\n",
    "    attention = attention[:len(decoded_word),:len(input_word)]\n",
    "    print(attention)\n",
    "    plt.imshow(attention,cmap='gray')\n",
    "    font_prop = FontProperties(fname='Lohit-Telugu.ttf', size=18)\n",
    "   \n",
    "    # plt.xlim(len(input_word))\n",
    "    # plt.ylim(len(input_word))\n",
    "    labels = []\n",
    "    for s in decoded_word:\n",
    "        labels.append(s)\n",
    "\n",
    "    plt.yticks(range(len(labels)),labels,fontproperties = font_prop)\n",
    "    plt.xticks(range(len(input_word)),input_word)\n",
    "    x+=1\n",
    "    \n",
    "wandb.log({\"heat_maps\":plt})\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Connectivity Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "random_indexes = []\n",
    "# outputs = []\n",
    "# inputs = []\n",
    "for i in range(9):\n",
    "    a = np.random.randint(0,len(test_outputs))\n",
    "    random_indexes.append(a)\n",
    "# fig = plt.figure(figsize = (15,15))\n",
    "x = 1\n",
    "for i in range(9):\n",
    "    accruracy , z, attention = beam_evaluate_single(test_input_tensor,test_outputs,random_indexes[i],1,enc_model,dec_model,'LSTM')\n",
    "    # print(attention)\n",
    "    attention = np.asarray(np.asarray(attention)[:,0,:])\n",
    "    s = attention.shape\n",
    "    attention = attention.reshape(s[0],s[2])[:,:s[0]]\n",
    "    # plt.subplot(3,3,x)\n",
    "    # print(attention.shape,attention)\n",
    "    for a, b in z:\n",
    "        decoded_word = a[1:-1]\n",
    "        expected_word = b \n",
    "    input_word = test_inputs[random_indexes[i]]\n",
    "    print(len(input_word),input_word)\n",
    "    print(len(decoded_word),decoded_word)\n",
    "    attention = attention[:len(decoded_word),:len(input_word)]\n",
    "    data = data_string2(attention,decoded_word,input_word)\n",
    "    print(data)\n",
    "    heatmap = TextualHeatmap(facet_titles = ['Vis'], show_meta=True)\n",
    "    heatmap.set_data(data)\n",
    "    heatmap.highlight(159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python370jvsc74a57bd0e2fb31f127fed1e79f709faf34890f85681c06e7e30d28a328e5c7500cdf6725",
   "display_name": "Python 3.7.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}