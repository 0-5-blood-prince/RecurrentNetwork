{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3927e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5007e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "# For limiting GPU VRAM used by process\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7500)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58285d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.getcwd() + os.sep + 'te' + os.sep + 'lexicons'\n",
    "train_path = dataset_path + os.sep + 'te.translit.sampled.train.tsv'\n",
    "valid_path = dataset_path + os.sep + 'te.translit.sampled.dev.tsv'\n",
    "test_path = dataset_path + os.sep + 'te.translit.sampled.test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6901b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = []\n",
    "train_outputs = []\n",
    "valid_inputs = []\n",
    "valid_outputs = []\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "input_chars = set()\n",
    "output_chars = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad91d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748bb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: (len(lines) - 1 )]:\n",
    "    out,inp,a = line.split('\\t')\n",
    "#     if not include_all and a!=1:\n",
    "#         continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     print(out,out[-1],inp,inp[-1])\n",
    "    out = \"\\t\" + out + \"\\n\"\n",
    "    train_inputs.append(inp)\n",
    "    train_outputs.append(out)\n",
    "    for char in inp:\n",
    "        if char not in input_chars:\n",
    "            input_chars.add(char)\n",
    "    for char in out:\n",
    "        if char not in output_chars:\n",
    "            output_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77599cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58550"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d3b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: (len(lines) - 1 )]:\n",
    "    out,inp,a = line.split('\\t')\n",
    "    if not include_all and a!=1:\n",
    "        continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     print(out,out[-1],inp,inp[-1])\n",
    "    out = \"\\t\" + out + \"\\n\"\n",
    "    valid_inputs.append(inp)\n",
    "    valid_outputs.append(out)\n",
    "#     for char in inp:\n",
    "#         if char not in input_chars:\n",
    "#             input_chars.add(char)\n",
    "#     for char in out:\n",
    "#         if char not in output_chars:\n",
    "#             output_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56668b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: (len(lines) - 1 )]:\n",
    "    out,inp,a = line.split('\\t')\n",
    "    if not include_all and a!=1:\n",
    "        continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     print(out,out[-1],inp,inp[-1])\n",
    "    out = \"\\t\" + out + \"\\n\"\n",
    "    test_inputs.append(inp)\n",
    "    test_outputs.append(out)\n",
    "#     for char in inp:\n",
    "#         if char not in input_chars:\n",
    "#             input_chars.add(char)\n",
    "#     for char in out:\n",
    "#         if char not in output_chars:\n",
    "#             output_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e91aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "input_chars = sorted(list(input_chars))\n",
    "print(input_chars)\n",
    "num_input_chars = len(input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9618b73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', 'ం', 'ః', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'క', 'ఖ', 'గ', 'ఘ', 'చ', 'ఛ', 'జ', 'ఝ', 'ఞ', 'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ఱ', 'ల', 'ళ', 'వ', 'శ', 'ష', 'స', 'హ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 'ృ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', '్', '\\u200c']\n"
     ]
    }
   ],
   "source": [
    "output_chars = sorted(list(output_chars))\n",
    "print(output_chars)\n",
    "num_output_chars = len(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aacbd5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 21 23\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_inputs)\n",
    "max_input_size = max([len(txt) for txt in train_inputs])\n",
    "max_valid_input_size = max([len(txt) for txt in valid_inputs])\n",
    "max_test_input_size = max([len(txt) for txt in test_inputs])\n",
    "print(max_input_size,max_valid_input_size,max_test_input_size)\n",
    "max_output_size = max([len(txt) for txt in  train_outputs])\n",
    "print(max_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a8b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "input_index = dict([(char, i+1) for i, char in enumerate(input_chars)])\n",
    "output_index = dict([(char, i+1) for i, char in enumerate(output_chars)])\n",
    "print(input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba93c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 1, '\\n': 2, 'ం': 3, 'ః': 4, 'అ': 5, 'ఆ': 6, 'ఇ': 7, 'ఈ': 8, 'ఉ': 9, 'ఊ': 10, 'ఋ': 11, 'ఎ': 12, 'ఏ': 13, 'ఐ': 14, 'ఒ': 15, 'ఓ': 16, 'ఔ': 17, 'క': 18, 'ఖ': 19, 'గ': 20, 'ఘ': 21, 'చ': 22, 'ఛ': 23, 'జ': 24, 'ఝ': 25, 'ఞ': 26, 'ట': 27, 'ఠ': 28, 'డ': 29, 'ఢ': 30, 'ణ': 31, 'త': 32, 'థ': 33, 'ద': 34, 'ధ': 35, 'న': 36, 'ప': 37, 'ఫ': 38, 'బ': 39, 'భ': 40, 'మ': 41, 'య': 42, 'ర': 43, 'ఱ': 44, 'ల': 45, 'ళ': 46, 'వ': 47, 'శ': 48, 'ష': 49, 'స': 50, 'హ': 51, 'ా': 52, 'ి': 53, 'ీ': 54, 'ు': 55, 'ూ': 56, 'ృ': 57, 'ె': 58, 'ే': 59, 'ై': 60, 'ొ': 61, 'ో': 62, 'ౌ': 63, '్': 64, '\\u200c': 65}\n"
     ]
    }
   ],
   "source": [
    "print(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f8f0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enoding in indexes of characters in the set\n",
    "def encode_index(inputs,index):\n",
    "    data = []\n",
    "    for i in range(len(inputs)):\n",
    "        a = np.zeros(len(inputs[i]))\n",
    "        j = 0\n",
    "        for char in inputs[i]:\n",
    "            a[j] = index[char]\n",
    "            j += 1\n",
    "        data.append(a)\n",
    "    data = np.asarray(data).astype(np.ndarray)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d8d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = []\n",
    "# for i in range(train_size):\n",
    "#     a = np.zeros(len(train_inputs[i]))\n",
    "#     j = 0\n",
    "#     for char in train_inputs[i]:\n",
    "#         a[j] = input_index[char]\n",
    "#         j += 1\n",
    "#     input_data.append(a)\n",
    "# input_data = np.asarray(input_data).astype(np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "093d2dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaitesh\\.conda\\envs\\tf2.4\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "input_data = encode_index(train_inputs,input_index)\n",
    "input_tensor = tf.ragged.constant(input_data).to_tensor()\n",
    "\n",
    "val_input_data = encode_index(valid_inputs,input_index)\n",
    "val_input_tensor = tf.ragged.constant(val_input_data).to_tensor()\n",
    "\n",
    "test_input_data = encode_index(test_inputs,input_index)\n",
    "test_input_tensor = tf.ragged.constant(test_input_data).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62c961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "# input_tensor=input_tensor.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# val_input_tensor=val_input_tensor.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# test_input_tensor=test_input_tensor.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "953483ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58550\n"
     ]
    }
   ],
   "source": [
    "print(len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "350640cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val__input_size = max([len(txt) for txt in valid_inputs])\n",
    "max_val_output_size = max([len(txt) for txt in  valid_outputs])\n",
    "max_test_input_size = max([len(txt) for txt in test_inputs])\n",
    "max_test_output_size = max([len(txt) for txt in  test_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b26ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = np.zeros(\n",
    "    (len(train_inputs), max_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(train_inputs), max_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "for i,target_text in enumerate(train_outputs):\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, output_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_output_data[i, t - 1, output_index[char]] = 1.0\n",
    "#     decoder_input_data[i, t + 1 :, output_index[\" \"]] = 1.0\n",
    "#     decoder_output_data[i, t:, output_index[\" \"]] = 1.0\n",
    "# print(decoder_input_data[0])\n",
    "decoder_input_data = np.argmax(decoder_input_data,axis=2).astype(dtype='float32')\n",
    "# decoder_output_data = np.argmax(decoder_output_data,axis=2).astype(dtype='float32')\n",
    "# decoder_input_data = tf.convert_to_tensor(decoder_input_data)\n",
    "# decoder_output_data = tf.convert_to_tensor(decoder_output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8681f530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  5.  3. 18. 53. 32.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.]\n",
      "[ 1. 13. 11.  9. 20.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data[0])\n",
    "print(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d32549f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_val_input_data = np.zeros(\n",
    "    (len(valid_inputs), max_val_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "decoder_val_output_data = np.zeros(\n",
    "    (len(valid_inputs), max_val_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "for i,target_text in enumerate(valid_outputs):\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_val_input_data[i, t, output_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_val_output_data[i, t - 1, output_index[char]] = 1.0\n",
    "#     decoder_input_data[i, t + 1 :, output_index[\" \"]] = 1.0\n",
    "#     decoder_output_data[i, t:, output_index[\" \"]] = 1.0\n",
    "decoder_val_input_data = np.argmax(decoder_val_input_data,axis=2).astype(dtype='float32')\n",
    "# decoder_val_output_data = np.argmax(decoder_val_output_data,axis=2).astype(dtype='float32')\n",
    "# decoder_val_input_data = tf.convert_to_tensor(decoder_val_input_data)\n",
    "# decoder_val_output_data = tf.convert_to_tensor(decoder_val_output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52f20581",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_test_input_data = np.zeros(\n",
    "    (len(test_inputs), max_test_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "decoder_test_output_data = np.zeros(\n",
    "    (len(test_inputs), max_test_output_size,num_output_chars+1), dtype=\"float32\"\n",
    ")\n",
    "for i,target_text in enumerate(test_outputs):\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_test_input_data[i, t, output_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_test_output_data[i, t - 1, output_index[char]] = 1.0\n",
    "#     decoder_input_data[i, t + 1 :, output_index[\" \"]] = 1.0\n",
    "#     decoder_output_data[i, t:, output_index[\" \"]] = 1.0\n",
    "decoder_test_input_data = np.argmax(decoder_test_input_data,axis=2).astype(dtype='float32')\n",
    "# decoder_test_output_data = np.argmax(decoder_test_output_data,axis=2).astype(dtype='float32')\n",
    "# decoder_test_input_data = tf.convert_to_tensor(decoder_test_input_data)\n",
    "# decoder_test_output_data = tf.convert_to_tensor(decoder_test_output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e50801eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charinput = tf.keras.Input(shape=(None,),name=\"input\")\n",
    "# embedding = tf.keras.layers.Embedding(num_input_chars,input_embed_size, name=\"embedding\")(charinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13b5339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Model(charinput,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b7e8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\"rmsprop\",\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "745cffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model.predict(input_data[0])\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66469fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "# x = K.placeholder(shape=(None, None, 2))\n",
    "# y = K.placeholder(shape=(2, 2))\n",
    "\n",
    "# print(K.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751cf97",
   "metadata": {},
   "source": [
    "# Adding Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa2157e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb688c59",
   "metadata": {},
   "source": [
    "# Sample Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b62a425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_model(input_embed_size , hidden_size):\n",
    "    charinput = tf.keras.Input(shape=(None,),dtype='float32',name=\"input\")\n",
    "    embedding = tf.keras.layers.Embedding(num_input_chars+1,input_embed_size, name=\"embedding\")(charinput)\n",
    "    \n",
    "    encoder = tf.keras.layers.LSTM(hidden_size, return_state=True,return_sequences=True )\n",
    "    encoder_outputs, state_h, state_c = encoder(embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_inputs = tf.keras.Input(shape=(None,),name=\"decoder_input\")\n",
    "    decoder_embedding = tf.keras.layers.Embedding(num_output_chars + 1,64, name=\"decoder_embedding\",mask_zero=True)(decoder_inputs)\n",
    "    \n",
    "    decoder_lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    print(encoder_outputs.shape)\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = tf.keras.layers.Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    decoder_dense = tf.keras.layers.Dense(num_output_chars + 1, activation=\"softmax\")\n",
    "    dense_time = tf.keras.layers.TimeDistributed(decoder_dense, name='time_distributed_layer')\n",
    "#     decoder_pred = dense_time(decoder_concat_input)\n",
    "    \n",
    "    decoder_outputs = dense_time(decoder_concat_input)\n",
    "    \n",
    "    model = tf.keras.Model([charinput,decoder_inputs],decoder_outputs)\n",
    "    \n",
    "    encoder_states_attn = [encoder_outputs,state_h,state_c]\n",
    "    encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "    # define inference decoder\n",
    "#     decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "#     attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "#     decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "#     decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "#     decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "#                           outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "    decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    encoder_inf_states = tf.keras.Input(shape=(None,hidden_size), name='encoder_inf_states')\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "    decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1, name='concat')([decoder_outputs, attn_inf_out])\n",
    "    decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h,decoder_state_input_c]\n",
    "                                   , [decoder_outputs] + decoder_states + [attn_inf_states])\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d476dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model, enc_model, dec_model = get_sample_model(16,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c71b19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model.compile(\n",
    "#     optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "# sample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cc12abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, char) for char, i in input_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in output_index.items())\n",
    "reverse_target_char_index[0] = ' '\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    sz  = input_seq.shape[0]\n",
    "    states_value = enc_model.predict(input_seq)\n",
    "    target_seq = np.zeros((sz,1,num_output_chars+1))\n",
    "    for i in range(sz):\n",
    "        target_seq[i, 0, output_index[\"\\t\"]] = 1.0\n",
    "    target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "    decoded_seqs = [\"\" for i in range(sz)]\n",
    "    j = 0\n",
    "    while j < max_output_size:\n",
    "        output_tokens, h, c, attn = dec_model.predict([target_seq] + states_value)\n",
    "\n",
    "#         print(output_tokens)\n",
    "        sampled_token_index = np.argmax(output_tokens[:, -1, :],axis=1)\n",
    "        target_seq = np.zeros((sz, 1, num_output_chars+1))\n",
    "        for i in range(sz):\n",
    "            sampled_char = reverse_target_char_index[sampled_token_index[i]]\n",
    "            decoded_seqs[i] += sampled_char\n",
    "            target_seq[i, 0, sampled_token_index[i]] = 1.0\n",
    "        target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "        # Update states\n",
    "        states_value[1] = h\n",
    "        states_value[2] = c\n",
    "        j+=1\n",
    "    output = [ (\"\\t\"+st.split('\\n')[0]+\"\\n\") for st in decoded_seqs]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b557dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = input_tensor.to_tensor()\n",
    "# dec_model.run_eagerly = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60cea938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seqid in range(10):\n",
    "#     input_seq = input_tensor[seqid:seqid+1]\n",
    "# #     print(input_seq.shape,input_tensor.shape)\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "#     print(\"-\")\n",
    "#     print(\"Input sentence:\", train_inputs[seqid])\n",
    "#     print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "962dfe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 25), dtype=float64, numpy=\n",
       "array([[ 1., 13., 11.,  9., 20.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "input_tensor[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83f42ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model.fit(\n",
    "#     [input_tensor,decoder_input_data],\n",
    "#     decoder_output_data,\n",
    "#     batch_size=64,\n",
    "#     epochs=5,\n",
    "#     validation_data=([val_input_tensor,decoder_val_input_data],decoder_val_output_data),\n",
    "#     shuffle=True,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b5d9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(data_tensor,data_output,k):\n",
    "#     crct = 0\n",
    "#     input_seq = data_tensor[:k]\n",
    "# #     print(input_seq.shape,input_tensor.shape)\n",
    "#     decoded_sentences = decode_sequence(input_seq)\n",
    "#     sts = data_output[:k]\n",
    "#     crct += np.sum(np.array(sts) == np.array(decoded_sentences))\n",
    "# #         print(crct/(seqid+1))\n",
    "# #         for st,d in zip(sts,decoded_sentences):\n",
    "# #             print(st+\"_o\")\n",
    "# #             print(d+\"_o\")\n",
    "#     return crct/k,zip(decoded_sentences,sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42d57cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = evaluate(test_input_tensor,test_outputs,len(test_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6acc5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model.save('sample_attention'+os.sep+'model')\n",
    "# enc_model.save('sample_attention'+os.sep+'enc')\n",
    "# dec_model.save('sample_attention'+os.sep+'dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a59bc",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20e8929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in output_index.items())\n",
    "reverse_target_char_index[0] = ' '\n",
    "\n",
    "def beam_decode(input_seq, beam_size, enc_model, dec_model, cell_type):\n",
    "    sz  = input_seq.shape[0]\n",
    "    attention = []\n",
    "    states_value = enc_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((sz,1,num_output_chars+1))\n",
    "    \n",
    "    for i in range(sz):\n",
    "        target_seq[i, 0, output_index[\"\\t\"]] = 1.0\n",
    "    \n",
    "    target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "    decoded_seqs = [\"\" for i in range(sz)]\n",
    "    if cell_type == 'LSTM':\n",
    "        output_tokens, h, c, attn = dec_model.predict([target_seq] + states_value)\n",
    "        states = [states_value[0],h,c]\n",
    "        attention.append(attn)\n",
    "    if cell_type == 'GRU' or cell_type == 'RNN':\n",
    "        output_tokens, h, attn = dec_model.predict([target_seq] + states_value)\n",
    "        states = [states_value[0],h]\n",
    "        attention.append(attn)\n",
    "    \n",
    "    sequences = []\n",
    "    for i in range(sz):\n",
    "        sequences.append([])\n",
    "    sampled_token_beam = np.argpartition(output_tokens[:, -1, :], -beam_size ,axis=1)[:,-beam_size:]\n",
    "    sampled_token_indexes = np.argmax(output_tokens[:, -1, :],axis=1)\n",
    "    for i in range(sz):\n",
    "        allcandidates = list()\n",
    "        for j in range(beam_size):\n",
    "            allcandidates.append(\n",
    "                    [ [ sampled_token_beam[i][j] ],\n",
    "                        -np.log( \n",
    "                        output_tokens[i][-1][sampled_token_beam[i][j]]),\n",
    "                        states ,\n",
    "                        False])\n",
    "        ordered = sorted(allcandidates, key=lambda tup:tup[1])\n",
    "        sequences[i] = ordered[:beam_size]\n",
    "        \n",
    "    target_seq = np.zeros((sz, beam_size, num_output_chars+1))\n",
    "    for i in range(sz):\n",
    "        for j in range(beam_size): \n",
    "            target_seq[i, j, sequences[i][j][0][-1]] = 1.0\n",
    "    target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "    it = 1\n",
    "    while it < max_output_size:\n",
    "        allcandidates = [list() for i in range(sz)]\n",
    "        for k in range(len(sequences[i])):\n",
    "            if cell_type == 'LSTM':\n",
    "                output_tokens, h, c, attn = dec_model.predict(\n",
    "                [target_seq[:,k]] + \n",
    "                sequences[i][k][2])\n",
    "                states = [states_value[0],h,c]\n",
    "                attention.append(attn)\n",
    "            if cell_type == 'GRU' or cell_type == 'RNN':\n",
    "                output_tokens, h, attn = dec_model.predict(\n",
    "                [target_seq[:,k]] + \n",
    "                sequences[i][k][2])\n",
    "                states = [states_value[0],h]\n",
    "                attention.append(attn)\n",
    "            sampled_token_beam = np.argpartition(output_tokens[:, -1, :], -beam_size ,axis=1)[:,-beam_size:]\n",
    "            sampled_token_indexes = np.argmax(output_tokens[:, -1, :],axis=1)\n",
    "            \n",
    "            for i in range(sz):\n",
    "                    if sequences[i][k][3]:\n",
    "                        allcandidates[i].append(\n",
    "                                [ sequences[i][k][0]+[ sampled_token_beam[i][j] ],\n",
    "                                 sequences[i][k][1],\n",
    "                                           states, True ])\n",
    "                        continue\n",
    "                    for j in range(beam_size):\n",
    "                        if reverse_target_char_index[sampled_token_beam[i][j]]=='\\n':\n",
    "                            allcandidates[i].append(\n",
    "                                [ sequences[i][k][0]+[ sampled_token_beam[i][j] ],\n",
    "                                 sequences[i][k][1]-np.log( \n",
    "                                     output_tokens[i][-1][sampled_token_beam[i][j]]),\n",
    "                                           states, True ])\n",
    "                        else:\n",
    "                            allcandidates[i].append(\n",
    "                            [ sequences[i][k][0]+[ sampled_token_beam[i][j] ],\n",
    "                             sequences[i][k][1]-np.log( \n",
    "                                 output_tokens[i][-1][sampled_token_beam[i][j]]),\n",
    "                                       states, False ])\n",
    "        for i in range(sz):\n",
    "            ordered = sorted(allcandidates[i], key=lambda tup:tup[1])\n",
    "            sequences[i] = ordered[:beam_size]\n",
    "        target_seq = np.zeros((sz, beam_size, num_output_chars+1))\n",
    "        for i in range(sz):\n",
    "            for j in range(beam_size): \n",
    "                target_seq[i, j, sequences[i][j][0][-1]] = 1.0\n",
    "        target_seq = np.argmax(target_seq,axis=2).astype('float32')\n",
    "        it+=1\n",
    "    output = []\n",
    "    for i in range(sz):\n",
    "        st = \"\"\n",
    "        for ind in sequences[i][0][0]:\n",
    "            st += reverse_target_char_index[ind]\n",
    "        output.append(\"\\t\"+st.split('\\n')[0]+\"\\n\")\n",
    "    return output , attention\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0075b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate(data_tensor,data_output,k,beam_size,enc_model, dec_model, cell_type):\n",
    "    crct = 0\n",
    "    input_seq = data_tensor[:k]\n",
    "    decoded_sentences, attention = beam_decode(input_seq,beam_size,enc_model, dec_model, cell_type)\n",
    "    sts = data_output[:k]\n",
    "    crct += np.sum(np.array(sts) == np.array(decoded_sentences))\n",
    "    return crct/k,zip(decoded_sentences,sts),attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "540c2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1, b1, attention = beam_evaluate(test_input_tensor,test_outputs,len(test_outputs),1,enc_model,dec_model,'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4883fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779eecba",
   "metadata": {},
   "source": [
    "# Attention color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "650b2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate_single(data_tensor,data_output,index,beam_size,enc_model, dec_model, cell_type):\n",
    "    crct = 0\n",
    "    input_seq = data_tensor[index:index+1]\n",
    "    decoded_sentences, attention = beam_decode(input_seq,beam_size,enc_model, dec_model, cell_type)\n",
    "    sts = data_output[index:index+1]\n",
    "    crct += np.sum(np.array(sts) == np.array(decoded_sentences))\n",
    "    return crct,zip(decoded_sentences,sts),attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83b187dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # coding: utf-8\n",
    "# from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# random_indexes = []\n",
    "# # outputs = []\n",
    "# # inputs = []\n",
    "# for i in range(9):\n",
    "#     a = np.random.randint(0,len(test_outputs))\n",
    "#     random_indexes.append(a)\n",
    "# fig = plt.figure(figsize = (15,15))\n",
    "# x = 1\n",
    "# for i in range(9):\n",
    "#     accruracy , z, attention = beam_evaluate_single(test_input_tensor,test_outputs,random_indexes[i],1,enc_model,dec_model,'LSTM')\n",
    "#     # print(attention)\n",
    "#     attention = np.asarray(np.asarray(attention)[:,0,:])\n",
    "#     s = attention.shape\n",
    "#     attention = attention.reshape(s[0],s[2])[:,:s[0]]\n",
    "#     plt.subplot(3,3,x)\n",
    "#     # print(attention.shape,attention)\n",
    "#     for a, b in z:\n",
    "#         decoded_word = a[1:-1]\n",
    "#         expected_word = b \n",
    "#     input_word = test_inputs[random_indexes[i]]\n",
    "#     print(len(input_word),input_word)\n",
    "#     print(len(decoded_word),decoded_word)\n",
    "#     attention = attention[:len(decoded_word),:len(input_word)]\n",
    "#     print(attention)\n",
    "#     plt.imshow(attention,cmap='gray')\n",
    "#     font_prop = FontProperties(fname='Lohit-Telugu.ttf', size=18)\n",
    "   \n",
    "#     # plt.xlim(len(input_word))\n",
    "#     # plt.ylim(len(input_word))\n",
    "#     labels = []\n",
    "#     for s in decoded_word:\n",
    "#         labels.append(s)\n",
    "\n",
    "#     plt.yticks(range(len(labels)),labels,fontproperties = font_prop)\n",
    "#     plt.xticks(range(len(input_word)),input_word)\n",
    "#     x+=1\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f1b6b",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d89312f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textualheatmap import TextualHeatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "092e0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def data_string1(attention,output,input):\n",
    "    ## attention shape len(output) x len(input)  \n",
    "    data = []\n",
    "    dic_list = []\n",
    "    for i in range(len(output)):\n",
    "        l = []\n",
    "        a = []\n",
    "        for j in range(len(input)):\n",
    "            a.append((-math.log(attention[i][j]),input[j]))\n",
    "        a.sort()\n",
    "        m = []\n",
    "        for j in range(4):\n",
    "            if a[j][0] <= 3:\n",
    "                m.append(a[j][1])\n",
    "        d = dict()\n",
    "        d[\"token\"] =  output[i]\n",
    "        d[\"meta\"] = m\n",
    "        d[\"heat\"] = [1.0]\n",
    "        d1 = dict()\n",
    "        d1[\"token\"] = ' '\n",
    "        d1[\"format\"] = False\n",
    "        dic_list.append(d)\n",
    "        dic_list.append(d1)\n",
    "    data.append(dic_list)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "435c1148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 1, '\\n': 2, 'ం': 3, 'ః': 4, 'అ': 5, 'ఆ': 6, 'ఇ': 7, 'ఈ': 8, 'ఉ': 9, 'ఊ': 10, 'ఋ': 11, 'ఎ': 12, 'ఏ': 13, 'ఐ': 14, 'ఒ': 15, 'ఓ': 16, 'ఔ': 17, 'క': 18, 'ఖ': 19, 'గ': 20, 'ఘ': 21, 'చ': 22, 'ఛ': 23, 'జ': 24, 'ఝ': 25, 'ఞ': 26, 'ట': 27, 'ఠ': 28, 'డ': 29, 'ఢ': 30, 'ణ': 31, 'త': 32, 'థ': 33, 'ద': 34, 'ధ': 35, 'న': 36, 'ప': 37, 'ఫ': 38, 'బ': 39, 'భ': 40, 'మ': 41, 'య': 42, 'ర': 43, 'ఱ': 44, 'ల': 45, 'ళ': 46, 'వ': 47, 'శ': 48, 'ష': 49, 'స': 50, 'హ': 51, 'ా': 52, 'ి': 53, 'ీ': 54, 'ు': 55, 'ూ': 56, 'ృ': 57, 'ె': 58, 'ే': 59, 'ై': 60, 'ొ': 61, 'ో': 62, 'ౌ': 63, '్': 64, '\\u200c': 65}\n"
     ]
    }
   ],
   "source": [
    "print(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec9bdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_string2(attention,output,input):\n",
    "    ## attention shape len(output) x len(input)  \n",
    "    data = []\n",
    "    dic_list = []\n",
    "    i = 0\n",
    "    while i < (len(output)):\n",
    "        token = output[i]\n",
    "        a = []\n",
    "        for j in range(len(input)):\n",
    "            a.append((-math.log(attention[i][j]),input[j]))\n",
    "        a.sort()\n",
    "        m = []\n",
    "        \n",
    "        mst = \"Core: \"\n",
    "        for j in range(4):\n",
    "            if a[j][0] <= 3:\n",
    "                mst = mst + a[j][1] + \" \"\n",
    "        m.append(mst)\n",
    "        while(  ( (i+1)<len(output) ) and ( output_index[output[i+1]] < 5 or output_index[output[i+1]] > 51) ):\n",
    "            i+=1\n",
    "            token += output[i]\n",
    "            a = []\n",
    "            for j in range(len(input)):\n",
    "                a.append((-math.log(attention[i][j]),input[j]))\n",
    "            a.sort()\n",
    "            mst = output[i] + \": \"\n",
    "            for j in range(4):\n",
    "                if a[j][0] <= 3:\n",
    "                    mst = mst + a[j][1] + \" \"\n",
    "            m.append(mst)\n",
    "\n",
    "        d = dict()\n",
    "        d[\"token\"] =  token\n",
    "        d[\"meta\"] = m\n",
    "        d[\"heat\"] = [1.0]\n",
    "        d1 = dict()\n",
    "        d1[\"token\"] = ' '\n",
    "        d1[\"format\"] = True\n",
    "        dic_list.append(d)\n",
    "        i+=1\n",
    "        dic_list.append(d1)\n",
    "    data.append(dic_list)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc1ec6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # coding: utf-8\n",
    "# from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# random_indexes = []\n",
    "# # outputs = []\n",
    "# # inputs = []\n",
    "# for i in range(9):\n",
    "#     a = np.random.randint(0,len(test_outputs))\n",
    "#     random_indexes.append(a)\n",
    "# fig = plt.figure(figsize = (15,15))\n",
    "# x = 1\n",
    "# for i in range(1):\n",
    "#     accruracy , z, attention = beam_evaluate_single(test_input_tensor,test_outputs,random_indexes[i],1,enc_model,dec_model,'LSTM')\n",
    "#     # print(attention)\n",
    "#     attention = np.asarray(np.asarray(attention)[:,0,:])\n",
    "#     s = attention.shape\n",
    "#     attention = attention.reshape(s[0],s[2])[:,:s[0]]\n",
    "#     # plt.subplot(3,3,x)\n",
    "#     # print(attention.shape,attention)\n",
    "#     for a, b in z:\n",
    "#         decoded_word = a[1:-1]\n",
    "#         expected_word = b \n",
    "#     input_word = test_inputs[random_indexes[i]]\n",
    "#     print(len(input_word),input_word)\n",
    "#     print(len(decoded_word),decoded_word)\n",
    "#     attention = attention[:len(decoded_word),:len(input_word)]\n",
    "#     data = data_string2(attention,decoded_word,input_word)\n",
    "#     print(data)\n",
    "#     heatmap = TextualHeatmap(facet_titles = ['Vis'], show_meta=True)\n",
    "#     heatmap.set_data(data)\n",
    "#     heatmap.highlight(159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e8b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d420bcfd",
   "metadata": {},
   "source": [
    "# Wandb Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10baf71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: mooizz (use `wandb login --relogin` to force relogin)\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Jaitesh/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login(key=\"866040d7d81f67025d43e7d50ecd83d54b6cf977\", relogin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "093bcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_word_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "         'beam_size' : {\n",
    "            'values' : [1]\n",
    "        },\n",
    "        'input_embed_size': {\n",
    "            'values' : [32, 64]\n",
    "        },\n",
    "        'hidden_size' : {\n",
    "            'values' : [128, 256, 512]\n",
    "        },\n",
    "        'cell_type' : {\n",
    "            'values' : ['GRU','LSTM','RNN']\n",
    "        },\n",
    "        'dropout' : {\n",
    "            'values' : [0,0.2]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eab47e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kf5qd0rd\n",
      "Sweep URL: https://wandb.ai/mooizz/Rec_dakhashina/sweeps/kf5qd0rd\n"
     ]
    }
   ],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, entity=\"mooizz\",project=\"Rec_dakhashina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3bc7ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Model(input_embed_size , hidden_size, cell_type, dropout):\n",
    "    charinput = tf.keras.Input(shape=(None,),dtype='float32',name=\"input\")\n",
    "    embedding = tf.keras.layers.Embedding(num_input_chars+1,input_embed_size, name=\"embedding\")(charinput)\n",
    "    \n",
    "    if cell_type == 'LSTM':\n",
    "        encoder = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True )\n",
    "        encoder_outputs, state_h, state_c = encoder(embedding)\n",
    "        encoder_states = [state_h, state_c]\n",
    "    if cell_type == 'RNN':\n",
    "        encoder = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, return_state=True )\n",
    "        encoder_outputs, rnn_state = encoder(embedding)\n",
    "    if cell_type == 'GRU':\n",
    "        encoder = tf.keras.layers.GRU(hidden_size, return_sequences=True, return_state=True )\n",
    "        encoder_outputs, gru_state = encoder(embedding)\n",
    "        \n",
    "    \n",
    "    decoder_inputs = tf.keras.Input(shape=(None,),name=\"decoder_input\")\n",
    "    decoder_embedding = tf.keras.layers.Embedding(num_output_chars + 1,64, name=\"decoder_embedding\",mask_zero=True)(decoder_inputs)\n",
    "    \n",
    "    if cell_type == 'LSTM':\n",
    "        decoder_lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    if cell_type == 'RNN':\n",
    "        decoder_rnn = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=rnn_state)\n",
    "    if cell_type == 'GRU':\n",
    "        decoder_gru = tf.keras.layers.GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _ = decoder_gru(decoder_embedding, initial_state=gru_state)\n",
    "    \n",
    "    # print(encoder_outputs.shape)\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = tf.keras.layers.Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    decoder_dense = tf.keras.layers.Dense(num_output_chars + 1, activation=\"softmax\")\n",
    "    dense_time = tf.keras.layers.TimeDistributed(decoder_dense, name='time_distributed_layer')\n",
    "#     decoder_pred = dense_time(decoder_concat_input)\n",
    "    \n",
    "    decoder_outputs = dense_time(decoder_concat_input)\n",
    "    \n",
    "    model = tf.keras.Model([charinput,decoder_inputs],decoder_outputs)\n",
    "    \n",
    "    \n",
    "    if cell_type == 'LSTM':\n",
    "        encoder_states_attn = [encoder_outputs,state_h,state_c]\n",
    "        encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "        # define inference decoder\n",
    "    #     decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "    #     attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "    #     decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "    #     decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "    #     decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "    #                           outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "\n",
    "        decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "        decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "        encoder_inf_states = tf.keras.Input(shape=(None,hidden_size), name='encoder_inf_states')\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "        attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "        decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1, name='concat')([decoder_outputs, attn_inf_out])\n",
    "        decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "        decoder_states = [state_h, state_c]\n",
    "\n",
    "        decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h,decoder_state_input_c]\n",
    "                                       , [decoder_outputs] + decoder_states + [attn_inf_states])\n",
    "    if cell_type == 'GRU':\n",
    "        encoder_states_attn = [encoder_outputs,gru_state]\n",
    "        \n",
    "        encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "        decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "        encoder_inf_states = tf.keras.Input(shape=(None,hidden_size), name='encoder_inf_states')\n",
    "        \n",
    "        decoder_outputs, state_h = decoder_gru(decoder_embedding, initial_state=decoder_state_input_h)\n",
    "        attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "        decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1, name='concat')([decoder_outputs, attn_inf_out])\n",
    "        decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "#         decoder_states = [state_h, state_c]\n",
    "\n",
    "        decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h]\n",
    "                                       , [decoder_outputs] + [state_h] + [attn_inf_states])\n",
    "    if cell_type == 'RNN':\n",
    "        encoder_states_attn = [encoder_outputs,rnn_state]\n",
    "        \n",
    "        encoder_model = tf.keras.Model(charinput, encoder_states_attn)\n",
    "        decoder_state_input_h = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_state_input_c = tf.keras.Input(shape=(hidden_size,))\n",
    "#         decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "        encoder_inf_states = tf.keras.Input(shape=(None,hidden_size), name='encoder_inf_states')\n",
    "        \n",
    "        decoder_outputs, state_h = decoder_rnn(decoder_embedding, initial_state=decoder_state_input_h)\n",
    "        attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_outputs])\n",
    "        decoder_inf_concat = tf.keras.layers.Concatenate(axis=-1, name='concat')([decoder_outputs, attn_inf_out])\n",
    "        decoder_outputs = tf.keras.layers.TimeDistributed(decoder_dense)(decoder_inf_concat)\n",
    "#         decoder_states = [state_h, state_c]\n",
    "\n",
    "        decoder_model = tf.keras.Model([decoder_inputs] + [encoder_inf_states,decoder_state_input_h]\n",
    "                                       , [decoder_outputs] + [state_h] + [attn_inf_states])\n",
    "        \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52f0d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_defaults = {\n",
    "        'epochs' : 10,\n",
    "        'batch_size' : 64,\n",
    "        'optimizer' : 'adam',\n",
    "        'beam_size' : 1,\n",
    "        'input_embed_size': 32,\n",
    "        'hidden_size' : 256,\n",
    "        'cell_type' : 'LSTM',\n",
    "        'dropout' : 0.2,\n",
    "    }\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "    model, enc_model, dec_model = get_Model(config.input_embed_size,config.hidden_size,\n",
    "                     config.cell_type,\n",
    "                     config.dropout)\n",
    "    model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "#     sample_model.summary()\n",
    "    EarlyStopCB = tf.keras.callbacks.EarlyStopping(patience=30, monitor='val_accuracy',\n",
    "                                                  restore_best_weights=True)\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    model.fit(\n",
    "        [input_tensor,decoder_input_data],\n",
    "        decoder_output_data,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epochs,\n",
    "        validation_data=(\n",
    "            [val_input_tensor,decoder_val_input_data],\n",
    "            decoder_val_output_data\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        callbacks=[WandbCallback(), EarlyStopCB])\n",
    "    beam_acc , _ , attention = beam_evaluate(val_input_tensor,valid_outputs,len(valid_outputs),config.beam_size,\n",
    "                                enc_model,\n",
    "                                dec_model,\n",
    "                                config.cell_type)\n",
    "    wandb.log({'val_word_accuracy' : beam_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e625837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: qmhh39q6 with config:\n",
      "wandb: \tbeam_size: 1\n",
      "wandb: \tcell_type: RNN\n",
      "wandb: \tdropout: 0.2\n",
      "wandb: \thidden_size: 128\n",
      "wandb: \tinput_embed_size: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">resilient-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/sweeps/kf5qd0rd\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/sweeps/kf5qd0rd</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/runs/qmhh39q6\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/runs/qmhh39q6</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\RecurrentNetwork\\wandb\\run-20210519_231701-qmhh39q6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaitesh\\.conda\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "915/915 [==============================] - 164s 178ms/step - loss: 0.9184 - accuracy: 0.3560 - val_loss: 0.2211 - val_accuracy: 0.7550\n",
      "Epoch 2/10\n",
      "915/915 [==============================] - 160s 174ms/step - loss: 0.1771 - accuracy: 0.7882 - val_loss: 0.1660 - val_accuracy: 0.7846\n",
      "Epoch 3/10\n",
      "915/915 [==============================] - 157s 172ms/step - loss: 0.1334 - accuracy: 0.8119 - val_loss: 0.1456 - val_accuracy: 0.7963\n",
      "Epoch 4/10\n",
      "915/915 [==============================] - 158s 173ms/step - loss: 0.1181 - accuracy: 0.8219 - val_loss: 0.1306 - val_accuracy: 0.8063\n",
      "Epoch 5/10\n",
      "915/915 [==============================] - 158s 173ms/step - loss: 0.1044 - accuracy: 0.8296 - val_loss: 0.1251 - val_accuracy: 0.8109\n",
      "Epoch 6/10\n",
      "915/915 [==============================] - 158s 173ms/step - loss: 0.0975 - accuracy: 0.8343 - val_loss: 0.1425 - val_accuracy: 0.8015\n",
      "Epoch 7/10\n",
      "915/915 [==============================] - 160s 174ms/step - loss: 0.0899 - accuracy: 0.8392 - val_loss: 0.1242 - val_accuracy: 0.8118\n",
      "Epoch 8/10\n",
      "915/915 [==============================] - 162s 177ms/step - loss: 0.0859 - accuracy: 0.8421 - val_loss: 0.1222 - val_accuracy: 0.8138\n",
      "Epoch 9/10\n",
      "915/915 [==============================] - 162s 177ms/step - loss: 0.0824 - accuracy: 0.8447 - val_loss: 0.1217 - val_accuracy: 0.8143\n",
      "Epoch 10/10\n",
      "915/915 [==============================] - 162s 178ms/step - loss: 0.0771 - accuracy: 0.8477 - val_loss: 0.1296 - val_accuracy: 0.8086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20108<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1.27MB of 1.27MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\RecurrentNetwork\\wandb\\run-20210519_231701-qmhh39q6\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\RecurrentNetwork\\wandb\\run-20210519_231701-qmhh39q6\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.07747</td></tr><tr><td>accuracy</td><td>0.84764</td></tr><tr><td>val_loss</td><td>0.12956</td></tr><tr><td>val_accuracy</td><td>0.80864</td></tr><tr><td>_runtime</td><td>1675</td></tr><tr><td>_timestamp</td><td>1621448096</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.1217</td></tr><tr><td>best_epoch</td><td>8</td></tr><tr><td>val_word_accuracy</td><td>0.46138</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▂▂▂▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▂▁▁▁▂</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇█▆███▇</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>val_word_accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">resilient-sweep-3</strong>: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/runs/qmhh39q6\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/runs/qmhh39q6</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: ioe06pur with config:\n",
      "wandb: \tbeam_size: 1\n",
      "wandb: \tcell_type: RNN\n",
      "wandb: \tdropout: 0\n",
      "wandb: \thidden_size: 256\n",
      "wandb: \tinput_embed_size: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">gallant-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/sweeps/kf5qd0rd\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/sweeps/kf5qd0rd</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/runs/ioe06pur\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/runs/ioe06pur</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\RecurrentNetwork\\wandb\\run-20210519_234500-ioe06pur</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "915/915 [==============================] - 157s 171ms/step - loss: 0.8503 - accuracy: 0.3920 - val_loss: 0.1561 - val_accuracy: 0.7893\n",
      "Epoch 2/10\n",
      "915/915 [==============================] - 155s 170ms/step - loss: 0.1478 - accuracy: 0.8047 - val_loss: 0.1280 - val_accuracy: 0.8088\n",
      "Epoch 3/10\n",
      "915/915 [==============================] - 157s 171ms/step - loss: 0.1107 - accuracy: 0.8264 - val_loss: 0.1159 - val_accuracy: 0.8151\n",
      "Epoch 4/10\n",
      "915/915 [==============================] - 157s 171ms/step - loss: 0.0953 - accuracy: 0.8365 - val_loss: 0.1185 - val_accuracy: 0.8148\n",
      "Epoch 5/10\n",
      "915/915 [==============================] - 155s 169ms/step - loss: 0.1007 - accuracy: 0.8337 - val_loss: 0.1141 - val_accuracy: 0.8154\n",
      "Epoch 6/10\n",
      "915/915 [==============================] - 157s 171ms/step - loss: 0.0792 - accuracy: 0.8477 - val_loss: 0.1089 - val_accuracy: 0.8208\n",
      "Epoch 7/10\n",
      "915/915 [==============================] - 159s 173ms/step - loss: 0.0711 - accuracy: 0.8524 - val_loss: 0.1124 - val_accuracy: 0.8216\n",
      "Epoch 8/10\n",
      "915/915 [==============================] - 155s 170ms/step - loss: 0.0676 - accuracy: 0.8542 - val_loss: 0.1096 - val_accuracy: 0.8231\n",
      "Epoch 9/10\n",
      "915/915 [==============================] - 156s 171ms/step - loss: 0.0619 - accuracy: 0.8585 - val_loss: 0.1118 - val_accuracy: 0.8239\n",
      "Epoch 10/10\n",
      "915/915 [==============================] - 156s 170ms/step - loss: 0.0575 - accuracy: 0.8613 - val_loss: 0.1147 - val_accuracy: 0.8220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26548<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 3.91MB of 3.91MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\RecurrentNetwork\\wandb\\run-20210519_234500-ioe06pur\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\RecurrentNetwork\\wandb\\run-20210519_234500-ioe06pur\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.05854</td></tr><tr><td>accuracy</td><td>0.86122</td></tr><tr><td>val_loss</td><td>0.11473</td></tr><tr><td>val_accuracy</td><td>0.822</td></tr><tr><td>_runtime</td><td>1637</td></tr><tr><td>_timestamp</td><td>1621449737</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.10887</td></tr><tr><td>best_epoch</td><td>5</td></tr><tr><td>val_word_accuracy</td><td>0.52666</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▂▂▂▂▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▂▁▁▂</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▆▇████</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>val_word_accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">gallant-sweep-4</strong>: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/runs/ioe06pur\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/runs/ioe06pur</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: aqb2dm1l with config:\n",
      "wandb: \tbeam_size: 1\n",
      "wandb: \tcell_type: LSTM\n",
      "wandb: \tdropout: 0.2\n",
      "wandb: \thidden_size: 256\n",
      "wandb: \tinput_embed_size: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">eager-sweep-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/sweeps/kf5qd0rd\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/sweeps/kf5qd0rd</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mooizz/Rec_dakhashina/runs/aqb2dm1l\" target=\"_blank\">https://wandb.ai/mooizz/Rec_dakhashina/runs/aqb2dm1l</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Jaitesh\\Desktop\\Courses\\DL\\RecurrentNetwork\\wandb\\run-20210520_001223-aqb2dm1l</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "915/915 [==============================] - 134s 144ms/step - loss: 0.9788 - accuracy: 0.3224 - val_loss: 0.1696 - val_accuracy: 0.7818\n",
      "Epoch 2/10\n",
      "348/915 [==========>...................] - ETA: 1:14 - loss: 0.1622 - accuracy: 0.7961"
     ]
    }
   ],
   "source": [
    "# wandb.agent(sweep_id, train)\n",
    "wandb.agent('kf5qd0rd', train, entity=\"mooizz\",project=\"Rec_dakhashina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffe5d0",
   "metadata": {},
   "source": [
    "# Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4832559",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, enc_model, dec_model = get_Model(config.input_embed_size,config.hidden_size,\n",
    "                     config.cell_type,\n",
    "                     config.dropout)\n",
    "best_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c824dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopCB = tf.keras.callbacks.EarlyStopping(patience=30, monitor='val_accuracy',\n",
    "                                                  restore_best_weights=True)\n",
    "tf.config.run_functions_eagerly(True)\n",
    "best_model.fit(\n",
    "        [input_tensor,decoder_input_data],\n",
    "        decoder_output_data,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epochs,\n",
    "        validation_data=(\n",
    "            [val_input_tensor,decoder_val_input_data],\n",
    "            decoder_val_output_data\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        callbacks=[WandbCallback(), EarlyStopCB]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, b1, attention = beam_evaluate(test_input_tensor,test_outputs,len(test_outputs),1,enc_model,dec_model,'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "random_indexes = []\n",
    "# outputs = []\n",
    "# inputs = []\n",
    "for i in range(9):\n",
    "    a = np.random.randint(0,len(test_outputs))\n",
    "    random_indexes.append(a)\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "x = 1\n",
    "for i in range(9):\n",
    "    accruracy , z, attention = beam_evaluate_single(test_input_tensor,test_outputs,random_indexes[i],1,enc_model,dec_model,'LSTM')\n",
    "    # print(attention)\n",
    "    attention = np.asarray(np.asarray(attention)[:,0,:])\n",
    "    s = attention.shape\n",
    "    attention = attention.reshape(s[0],s[2])[:,:s[0]]\n",
    "    plt.subplot(3,3,x)\n",
    "    # print(attention.shape,attention)\n",
    "    for a, b in z:\n",
    "        decoded_word = a[1:-1]\n",
    "        expected_word = b \n",
    "    input_word = test_inputs[random_indexes[i]]\n",
    "    print(len(input_word),input_word)\n",
    "    print(len(decoded_word),decoded_word)\n",
    "    attention = attention[:len(decoded_word),:len(input_word)]\n",
    "    print(attention)\n",
    "    plt.imshow(attention,cmap='gray')\n",
    "    font_prop = FontProperties(fname='Lohit-Telugu.ttf', size=18)\n",
    "   \n",
    "    # plt.xlim(len(input_word))\n",
    "    # plt.ylim(len(input_word))\n",
    "    labels = []\n",
    "    for s in decoded_word:\n",
    "        labels.append(s)\n",
    "\n",
    "    plt.yticks(range(len(labels)),labels,fontproperties = font_prop)\n",
    "    plt.xticks(range(len(input_word)),input_word)\n",
    "    x+=1\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "random_indexes = []\n",
    "# outputs = []\n",
    "# inputs = []\n",
    "for i in range(9):\n",
    "    a = np.random.randint(0,len(test_outputs))\n",
    "    random_indexes.append(a)\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "x = 1\n",
    "for i in range(1):\n",
    "    accruracy , z, attention = beam_evaluate_single(test_input_tensor,test_outputs,random_indexes[i],1,enc_model,dec_model,'LSTM')\n",
    "    # print(attention)\n",
    "    attention = np.asarray(np.asarray(attention)[:,0,:])\n",
    "    s = attention.shape\n",
    "    attention = attention.reshape(s[0],s[2])[:,:s[0]]\n",
    "    # plt.subplot(3,3,x)\n",
    "    # print(attention.shape,attention)\n",
    "    for a, b in z:\n",
    "        decoded_word = a[1:-1]\n",
    "        expected_word = b \n",
    "    input_word = test_inputs[random_indexes[i]]\n",
    "    print(len(input_word),input_word)\n",
    "    print(len(decoded_word),decoded_word)\n",
    "    attention = attention[:len(decoded_word),:len(input_word)]\n",
    "    data = data_string2(attention,decoded_word,input_word)\n",
    "    print(data)\n",
    "    heatmap = TextualHeatmap(facet_titles = ['Vis'], show_meta=True)\n",
    "    heatmap.set_data(data)\n",
    "    heatmap.highlight(159)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
